{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00730149-9b1a-4f1a-90ff-d247afd2eb8e",
   "metadata": {},
   "source": [
    "# Tweet Sentiment Classification\n",
    "\n",
    "Build and train models to classify tweet sentiment as positive or negative using `Logistic Regression` and `Naïve Bayes` classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6f0c5fd5-3029-4c7e-840b-110eed0904d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from string import punctuation\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import twitter_samples, stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d52232-5944-47a2-98a9-868edc6a9aa0",
   "metadata": {},
   "source": [
    "## Download and explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "33c725ef-3615-4032-9d53-1103262e3e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download twitter samples and unzip \n",
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0328a375-f28e-4a57-b0b0-0398d458fda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load postive and negative tweets from JSON to list\n",
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7047374f-c557-4279-893e-f418fb77bb21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 5000)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(positive_tweets), len(negative_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ce040545-6b0e-41a5-a666-e586b2b708ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Tweets...\n",
      "Tweet 1:  #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "Tweet 2:  @Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!\n",
      "Tweet 3:  @DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!\n",
      "Tweet 4:  @97sides CONGRATS :)\n",
      "Tweet 5:  yeaaaah yippppy!!!  my accnt verified rqst has succeed got a blue tick mark on my fb profile :) in 15 days\n",
      "\n",
      "Negative Tweets...\n",
      "Tweet 1:  hopeless for tmr :(\n",
      "Tweet 2:  Everything in the kids section of IKEA is so cute. Shame I'm nearly 19 in 2 months :(\n",
      "Tweet 3:  @Hegelbon That heart sliding into the waste basket. :(\n",
      "Tweet 4:  “@ketchBurning: I hate Japanese call him \"bani\" :( :(”\n",
      "\n",
      "Me too\n",
      "Tweet 5:  Dang starting next week I have \"work\" :(\n"
     ]
    }
   ],
   "source": [
    "# View few samples tweets\n",
    "print('Positive Tweets...')\n",
    "for i, tweet in enumerate(positive_tweets[:5]): print(f'Tweet {i+1}: ', tweet)\n",
    "print('\\nNegative Tweets...')\n",
    "for i, tweet in enumerate(negative_tweets[:5]): print(f'Tweet {i+1}: ', tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fc5ce6-da91-4745-b285-a4304cee89e5",
   "metadata": {},
   "source": [
    "## Preprocess data\n",
    "Tweets needs to be cleaned and encoded to make them ready for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b07e6df8-e708-4f58-8991-b9be4def2368",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download NLTK tokenizer model and stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Get list of stop words and punctuations\n",
    "stop_words = stopwords.words('english')\n",
    "punctuations = list(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c6fee6de-dede-47f5-8a96-a680f36abcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_tokenize_tweet(tweet, stop_words, punctuations):\n",
    "    '''\n",
    "    Returns a clean tokenized representation of a tweet\n",
    "    1) Remove twitter handles, hash-tags and hyper-links\n",
    "    2) Convert every tweet to lowercase\n",
    "    3) Remove punctuation\n",
    "    4) Remove stop words\n",
    "    5) Apply stemming\n",
    "    '''\n",
    "    # Remove handles\n",
    "    clean_tweet = re.sub('@[a-zA-Z0-9_]+', \n",
    "                         '', tweet, flags=re.MULTILINE)\n",
    "    # Remove hashtags\n",
    "    clean_tweet = re.sub('#[a-zA-Z0-9_]+', \n",
    "                         '', clean_tweet, flags=re.MULTILINE) \n",
    "    # Remove hyperlinks\n",
    "    clean_tweet = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', \n",
    "                  '', clean_tweet, flags=re.MULTILINE)\n",
    "    # Covert to lowercase\n",
    "    clean_tweet = clean_tweet.lower()\n",
    "    # Split tweet in word tokens\n",
    "    tweet_tokens = word_tokenize(clean_tweet)\n",
    "    # Remove stop-words and punctuation\n",
    "    tweet_tokens = [token for token in tweet_tokens \n",
    "                    if token not in stop_words and\n",
    "                       token not in punctuations]\n",
    "    # Apply stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tweet_tokens = [stemmer.stem(token) for token in tweet_tokens] \n",
    "    return tweet_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "64bc8192-f0a8-41d5-81e2-1c7d2c294d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all tweets, clean and tokenize them\n",
    "clean_positive_tweets = [clean_and_tokenize_tweet(tweet, stop_words, punctuations) \n",
    "                         for tweet in positive_tweets[:5]]\n",
    "clean_negative_tweets = [clean_and_tokenize_tweet(tweet, stop_words, punctuations) \n",
    "                         for tweet in negative_tweets[:5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fb36f7-31c8-4d34-8bf4-fd274b14d711",
   "metadata": {},
   "source": [
    "### Encode tweets and etract features\n",
    "1) Build vocabulary (unique word list)\n",
    "2) Build frequency dictionary for positive and negative tweets\n",
    "3) Encode each tweet in corpus as [bias (always 1), sum-postive-freq, sum-postive-freq]\n",
    "4) build a matrix containing all encoded tweets and a list containg corresponding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd9151b-6bc0-4ab0-97b4-a2de197707c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "168e8549-0637-4d6b-869e-ff424572817b",
   "metadata": {},
   "source": [
    "### Split data in training (80%) and testing (20%) sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8035b192-ac14-4a77-ae13-6fb297324987",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11abc1da-f7b8-4d63-8d73-7d99f09b9547",
   "metadata": {},
   "source": [
    "## Logistics Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007dc18b-0b07-4c8c-b06c-023eeaf35847",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e63ae7c-ae12-4eaa-8c48-eb34dc12b454",
   "metadata": {},
   "source": [
    "## Naive Bays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ac0f09-2222-4418-9920-244fceed7967",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
